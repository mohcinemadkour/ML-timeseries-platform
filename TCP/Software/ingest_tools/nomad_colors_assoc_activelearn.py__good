#!/usr/bin/env python
"""
This code is to explore using active learning to build a
better ASAS <-> nomad source color association classifier.

This uses output .arff from get_colors_for_tutor_sources.py when using:
        best_nomad_sources = GetColorsUsingNomad.generate_nomad_tutor_source_associations(projid=126,
        pkl_fpath=pkl_fpath,
        do_store_nomad_sources_for_classifier=True)

This arrff has the form:
@RELATION ts
@ATTRIBUTE dist NUMERIC
@ATTRIBUTE j_acvs_nomad NUMERIC
@ATTRIBUTE h_acvs_nomad NUMERIC
@ATTRIBUTE k_acvs_nomad NUMERIC
@ATTRIBUTE jk_acvs_nomad NUMERIC
@ATTRIBUTE v_tutor_nomad NUMERIC
@ATTRIBUTE class {'match','not'}
@data

"""
import sys, os
from rpy2.robjects.packages import importr
from rpy2 import robjects
import numpy

# These are sources which were in the "test_withsrcid.arff" file,
#    sources which are not classified by the original hardcoded classifier
#    and I now pretend are not decided by some previous active-learning user.
#     - I also made sure there were no missing-value attributes in these lines


class Nomad_Colors_Assoc_AL:
    """ Class for doing the active learning for classifier which associates
    nomad sources to ASAS sources using color & distance based features.

    This is related to get_colors_for_tutor_source.py
    """
    def __init__(self, pars={}):
        self.pars = pars

        algorithms_dirpath = os.path.abspath(os.environ.get("TCP_DIR") + 'Algorithms/')
        sys.path.append(algorithms_dirpath)

        import rpy2_classifiers
        self.rc = rpy2_classifiers.Rpy2Classifier(algorithms_dirpath=algorithms_dirpath)



    def load_arff(self, arff_str, skip_missingval_lines=False, fill_arff_rows=False):
        """ Parse existing arff with Nomad/ASAS color based features
        """
        data_dict = self.rc.parse_full_arff(arff_str=arff_str, skip_missingval_lines=skip_missingval_lines, fill_arff_rows=fill_arff_rows)
        return data_dict



    def actlearn_randomforest(self, traindata_dict={},
                              testdata_dict={},
                              do_ignore_NA_features=False,
                              ntrees=1000, mtry=25,
                              nfolds=10, nodesize=5,
                              num_srcs_for_users=100,
                              random_seed=0,
                              both_user_match_srcid_bool=[],
                              actlearn_sources_freqsignifs=[]):
        """
        This was adapted from:

           rpy2_classifiers.py:actlearn_randomforest():
                  - Train a randomForest() R classifier : Taken from class_cv.R : rf.cv (L40)

        """
        if do_ignore_NA_features:
            print "actlearn_randomforest():: do_ignore_NA_features==True not implemented because obsolete"
            raise



        train_featname_longfeatval_dict = traindata_dict['featname_longfeatval_dict']
        for feat_name, feat_longlist in train_featname_longfeatval_dict.iteritems():
            #if feat_name == 'dist':
            #    import pdb; pdb.set_trace()
            #    print

            train_featname_longfeatval_dict[feat_name] = robjects.FloatVector(feat_longlist)
        traindata_dict['features'] = robjects.r['data.frame'](**train_featname_longfeatval_dict)
        traindata_dict['classes'] = robjects.StrVector(traindata_dict['class_list'])

        robjects.globalenv['xtr'] = traindata_dict['features']
        robjects.globalenv['ytr'] = traindata_dict['classes']
        
        test_featname_longfeatval_dict = testdata_dict['featname_longfeatval_dict']
        for feat_name, feat_longlist in test_featname_longfeatval_dict.iteritems():
            #if feat_name == 'dist':
            #    import pdb; pdb.set_trace()
            #    print
            test_featname_longfeatval_dict[feat_name] = robjects.FloatVector(feat_longlist)
        testdata_dict['features'] = robjects.r['data.frame'](**test_featname_longfeatval_dict)
        testdata_dict['classes'] = robjects.StrVector(testdata_dict['class_list'])

        robjects.globalenv['xte'] = testdata_dict['features']
        robjects.globalenv['yte'] = testdata_dict['classes']

        #import pdb; pdb.set_trace()
        #print

        #robjects.globalenv['instep'] = robjects.IntVector(actlearn_used_srcids_indicies)
        #robjects.globalenv['incl_tr'] = robjects.BoolVector(both_user_match_srcid_bool)
        robjects.globalenv['actlearn_sources_freqsignifs'] = robjects.FloatVector(actlearn_sources_freqsignifs)
        robjects.globalenv['both_user_match_srcid_bool'] = robjects.BoolVector(both_user_match_srcid_bool)

        #for class_name in testdata_dict['class_list']:
        #    if (('algol' in class_name.lower()) or ('persei' in class_name.lower())):
        #        print '!', class_name

        nparts = 4

        r_str  = '''
    cat("In R code\n")
    random_seed = %d
    set.seed(random_seed)

    m=%d

    ntrees=%d
    mtry=%d
    nfolds=%d
    nodesize=%d

    nparts=%d

    ytr = class.debos(ytr)

    n.tr = length(ytr) # number of training data
    n.te = dim(xte)[1] # number of test data

    if(is.null(mtry)){ mtry = ceiling(sqrt(dim(xtr)[2]))} # set mtry
    ## ## ## ## ## ## The following builds the proximity matrix and Active-learn derived features in an iterative manner:
    n_p = floor(n.te / nparts) # KLUDGE: misses 1 if not evenly divisable
    ### First iteration (ii=1), so that rho3 is declared:
    set.seed(random_seed)
    rf_clfr = randomForest(x=xtr,y=ytr,ntrees=ntrees,mtry=mtry,proximity=TRUE,nodesize=nodesize, keep.forest=TRUE)
        ''' % (random_seed, num_srcs_for_users, ntrees, mtry, nfolds, nodesize, nparts) #, nodesize)
        classifier_out = robjects.r(r_str)

        ### NOTE: I use these lists of arrays in hopes of eventually parallelizing this bit.
        ### even though I am just filling a triangle (half), I just create a nparts x nparts list of lists:
        ###              prox_list[i][j] triangle where j >= i ; contains 2D arrays
        prox_list = []
        votes_list = [] # length n_parts 
        for i in range(0, nparts + 1):
            votes_list.append([])
            prox_list.append([])
            for j in range(0, nparts + 1):
                prox_list[i].append([])

        ###loop:   0,1 0,2 0,3 0,tr  1,2 1,3 1,tr   2,3 2,tr
        for i in range(0, nparts):
            for j in range(i, nparts + 1):
                print i, j, nparts + 1
                r_str  = '''
    # starts at 0
    i=%d
    j=%d
    # xte_part contains the i data at the bottom or first section of rows, j data in the appended data
    xte_bot = xte[((i*n_p)+1):((i+1)*n_p),]

    if(j == nparts){
      xte_top = xtr
    } else {
      xte_top = xte[((j*n_p)+1):((j+1)*n_p),]
    }

    pr = predict(rf_clfr, newdata=rbind(xte_bot,xte_top), proximity=TRUE, norm.votes=FALSE, type='vote', predict.all=TRUE)

    rho_bot = pr$proximity[1:n_p,1:n_p]

    if (j == nparts){
      # this is the case which is convolved with the trainingset
      #rho_top = pr$proximity[(n_p+1):(2*n_p),-(1:n_p)]
      rho_top = pr$proximity[1:n_p,-(1:n_p)]
    } else {
      rho_top = pr$proximity[(n_p+1):(2*n_p),1:n_p]
    }

    if((i == (nparts-1)) & (j == nparts)){
      rho_tr_cross = pr$proximity[-(1:n_p),-(1:n_p)]
    }
    votes_bot = pr$predicted$aggregate[1:n_p,]
                ''' % (i, j)
                out = robjects.r(r_str)
                #import pdb; pdb.set_trace()
                #print
                ##### I think the only time rho_bot needs to be pr$proximity[1:n_p,-(1:n_p)] is when j=i=nparts
                #if (i == 4) and (j == 4):
                #    import pdb; pdb.set_trace()
                #    print

                #if j == i+1:
                #    ### This is the first entry into inner j loop
                #    votes_list[i] = numpy.array(robjects.r("votes_bot"))
                #    prox_list[i][i] = numpy.array(robjects.r("rho_bot"))
                #if (i == 0) and (j == 4):
                #    import matplotlib.pyplot as plt
                #    import numpy as np
                #    fig = plt.figure()
                #    ax = fig.add_subplot(111)
                #    #data = numpy.array(robjects.r("pr$proximity"))
                #    data = numpy.array(robjects.r("rho_top"))
                #    #data = numpy.array(robjects.r("rho_bot")) # this is just the symmetric i=0,j=0 square
                #    cax = ax.imshow(data, interpolation='nearest')
                #    plt.show()
                #    import pdb; pdb.set_trace()
                #    print


                if j == i:
                    ### This is the first entry into inner j loop
                    votes_list[i] = numpy.array(robjects.r("votes_bot"))
                    prox_list[i][i] = numpy.array(robjects.r("rho_bot"))
                else:
                    prox_list[i][j] = numpy.array(robjects.r("rho_top"))

                if ((j==nparts) and (i==nparts-1)):
                    prox_list[nparts][nparts] = numpy.array(robjects.r("rho_tr_cross")) # final corner train x train prox matrix

        nte = robjects.r("n.te")[0]
        ntr = robjects.r("n.tr")[0]
        n_p = robjects.r("n_p")[0]
        
        prox_arr = numpy.zeros((nte,nte + ntr))
        for i in range(0, nparts):
            #for j in range(i+1, nparts + 1):
            for j in range(i, nparts + 1):
                print i, j, nparts, prox_list[i][j].shape
                if len(prox_list[i][j]) > 0:
                    if j < nparts:
                        prox_arr[(j*n_p):(j+1)*n_p,(i*n_p):(i+1)*n_p] = prox_list[i][j] # dont do when j >= nparts
                        if j != i:
                            prox_arr[(i*n_p):(i+1)*n_p,(j*n_p):(j+1)*n_p] = prox_list[i][j].T
                    else:
                        prox_arr[(i*n_p):(i+1)*n_p,(j*n_p):] = prox_list[i][j] # dont do when j >= nparts
                        
                    #else:
                    #    
                    #if j != i:
                    #    if j == nparts:
                    #        #import pdb; pdb.set_trace()
                    #        #print
                    #        prox_arr[(i*n_p):(i+1)*n_p,(j*n_p):] = prox_list[i][j]#    .T # sticks wrong place in x
                    #    else:
                    #        prox_arr[(i*n_p):(i+1)*n_p,(j*n_p):(j+1)*n_p] = prox_list[i][j].T # sticks wrong place in x
                #import matplotlib.pyplot as plt
                #import numpy as np
                #fig = plt.figure()
                #ax = fig.add_subplot(111)
                #data = prox_arr
                #cax = ax.imshow(data, interpolation='nearest')
                #plt.show()
                #import pdb; pdb.set_trace()
                #print
        # Add the training data part of proximitry matrix:
        """
        i = nparts
        for j in range(0, nparts):
            #prox_arr[0:n_p,0:n_p] = prox_list[i][j]
            if len(prox_list[i][j]) > 0:
                #prox_arr[(i*n_p):(i+1)*n_p,(j*n_p):(j+1)*n_p] = prox_list[i][j] # sticks wrong place in x
                prox_arr[(j*n_p):(j+1)*n_p,(i*n_p):(i+1)*n_p] = prox_list[i][j]

                import matplotlib.pyplot as plt
                import numpy as np
                fig = plt.figure()
                ax = fig.add_subplot(111)
                data = prox_arr
                cax = ax.imshow(data, interpolation='nearest')
                plt.show()
                import pdb; pdb.set_trace()
                print
        """
        # TODO: define a matrix of (nte + ntr) x (nte + ntr) and fill it up
        #    - use compressed sparse matrix?  see what memory gain?
        #    - will want to transpose add this sparse matrix to create a full proximity matrix
        # TODO: extend the votes list
        # TODO: do the n.bar, p.hat, err.decr  calculations.
        
        import matplotlib.pyplot as plt
        import numpy as np
        fig = plt.figure()
        ax = fig.add_subplot(111)
        data = prox_arr
        cax = ax.imshow(data, interpolation='nearest')
        plt.show()
        import pdb; pdb.set_trace()
        print

        #import matplotlib.pyplot as plt
        #import numpy as np
        #fig = plt.figure()
        #ax = fig.add_subplot(111)
        #data = numpy.array(robjects.r("pr3$proximity"))
        #cax = ax.imshow(data, interpolation='nearest')
        #plt.show()
        

        #robjects.globalenv['pred_forconfmat']
        #robjects.r("rf_clfr$classes")

        possible_classes = robjects.r("rf_clfr$classes")

        actlearn_tups = []
        #  Nice and kludgey.  Could do this in R if I knew it a bit better
        #for i, srcid in enumerate(data_dict['srcid_list']):

        for i in robjects.globalenv['select']:
            # I think the robjects.globalenv['select'] R array has an index starting at i=1
            #   so this means if R array gives i=999, then this translates srcid_list[i=998]
            #   so this means if R array gives i=1, then this translates srcid_list[i=0]
            srcid = testdata_dict['srcid_list'][i-1]# index is python so starts at 0
            actlearn_tups.append((int(srcid), robjects.globalenv['err.decr'][i-1]))# I tested this, i starts at 0, 2012-03-12 dstarr confirmed


        #import pdb; pdb.set_trace()
        #print
        allsrc_tups = []
        everyclass_tups = []
        trainset_everyclass_tups = []
        #  Nice and kludgey.  Could do this in R if I knew it a bit better
        for i, srcid in enumerate(testdata_dict['srcid_list']):
            tups_list = zip(list(robjects.r("rf_clfr$test$votes[%d,]" % (i+1))),  possible_classes)
            tups_list.sort(reverse=True)
            for j in xrange(len(tups_list)):
                # This stores the prob ordered classifications, for top 3 classes, and seperately for all classes:
                if j < 3:
                    allsrc_tups.append((int(srcid), j, tups_list[j][0], tups_list[j][1]))
                everyclass_tups.append((int(srcid), j, tups_list[j][0], tups_list[j][1]))

        # # # This is just needed for filling the ASAS catalog tables:
        for i, srcid in enumerate(traindata_dict['srcid_list']):
            tups_list = zip(list(robjects.r("rf_applied_to_train$test$votes[%d,]" % (i+1))),  possible_classes)
            tups_list.sort(reverse=True)
            for j in xrange(len(tups_list)):
                trainset_everyclass_tups.append((int(srcid), j, tups_list[j][0], tups_list[j][1]))
        # # #

        #import pdb; pdb.set_trace()
        #print

        return {'al_probis_match':list(robjects.r('rf_clfr$test$votes[select,][,"match"]')),
                'al_probis_not':list(robjects.r('rf_clfr$test$votes[select,][,"not"]')),
                'al_deltaV':[robjects.globalenv['err.decr'][i-1] for i in list(robjects.globalenv['select'])],
                'al_srcid':[testdata_dict['srcid_list'][i-1] for i in list(robjects.globalenv['select'])],
            }
        """
        return {'actlearn_tups':actlearn_tups,
                'allsrc_tups':allsrc_tups,
                'everyclass_tups':everyclass_tups,
                'trainset_everyclass_tups':trainset_everyclass_tups,
                'py_obj':classifier_out,
                'r_name':'rf_clfr',
                'select':robjects.globalenv['select'],
                'select.pred':robjects.r("rf_clfr$test$predicted[select]"),
                'select.predprob':robjects.r("rf_clfr$test$votes[select,]"),
                'err.decr':robjects.globalenv['err.decr'],
                'all.pred':robjects.r("rf_clfr$test$predicted"),
                'all.predprob':robjects.r("rf_clfr$test$votes"),
                'possible_classes':possible_classes,
                'all_top_prob':robjects.r("apply(rf_clfr$test$votes,1,max)"),
                }
        """


    def main(self):
        """ Main method for initially prototyping this class.
        """
        train_fpath = '/home/dstarr/scratch/nomad_asas_acvs_classifier/train_chosen.arff'
        test_fpath  = '/home/dstarr/scratch/nomad_asas_acvs_classifier/notchosen_withclass_withsrcid.arff' #initialcase_withsrcid.arff'

        i_iter = 5
        n_test_to_sample=1000 #40000
        num_srcs_for_users=10 #1000

        random_seed = 1234

        ##### KLUDGE: initially we exclude all sources which have missing attributes
        #      - so we can do Active learning with imputation
        #      - generally the sources which have missing attributes are non-matches.
        train_lines = open(train_fpath).read().split('\n')
        train_lines2 = []
        for line in train_lines:
            if len(line) == 0:
                continue
            if '?' not in line:
                train_lines2.append(line)
        train_str = '\n'.join(train_lines2)

        test_lines = open(test_fpath).read().split('\n')
        test_lines2 = []
        for line in test_lines:
            # each source in this arff has '?' for a class, so we allow for that, but still skip missing attribs
            if line.count('?') <= 0:
                test_lines2.append(line)
        test_str = '\n'.join(test_lines2)
        #####

        # # # # # KLUDGE: we also give unique source_id names that represent NN rank and TUTOR srcid:
        test_lines2 = []
        test_str_split = test_str.split('\n')
        for i, line in enumerate(test_str_split):
            if len(line) == 0:
                new_line = line
            elif line[0] == '@':
                new_line = line
                if line.lower() == '@data':
                    test_lines2.append(new_line)
                    break
            test_lines2.append(new_line)
        i_header_end = i

        import random
        #for i, line in enumerate(random.sample(test_str_split[i_header_end + 1:], n_test_to_sample)):
        for i, line in enumerate(test_str_split[i_header_end + 1:n_test_to_sample + i_header_end + 1]):
            elems = line.split(',')
            try:
                srcid = "%d%0.2d" % (int(elems[0]), int(elems[2]))
            except:
                print 'EXCEPT:', elems, i, line
                import pdb; pdb.set_trace()
                print
            new_line = "%s,%s" % (srcid, ','.join(elems[1:]))
            test_lines2.append(new_line)
        test_str = '\n'.join(test_lines2)


        # # # # #
        
        traindata_dict = self.load_arff(train_str)
        testdata_dict = self.load_arff(test_str, skip_missingval_lines=False, fill_arff_rows=True)




        # TODO: ignore sources which have missing values, for now.
        #    -> TODO: we will train a general RF classifier which allows missing-values in test data


        # these are for all sources (train and test):

        ### The following lists contain a combination of sources:
        ###     - all training sources, and previous Active Learning trained sources
        ###     - sources which users recently attempted to classify for an
        ###         active learning iteration, including unsure/non-consensus ([False]) sources
        both_user_match_srcid_bool = [True] * len(traindata_dict['featname_longfeatval_dict']['dist'])# list of [True]
        actlearn_sources_freqsignifs = traindata_dict['featname_longfeatval_dict']['dist'] # list of cost metrics

        # ** TODO: need to add some ambiguoius sources?
        #       - ??? from the test_withsrcid.arff file?



        class_dict = self.actlearn_randomforest(traindata_dict=traindata_dict,
                                                     testdata_dict=testdata_dict,
                                                     mtry=5,
                                                     ntrees=500,
                                                     nodesize=5,
                                                     num_srcs_for_users=num_srcs_for_users,
                                                     random_seed=random_seed,
                                                     both_user_match_srcid_bool=both_user_match_srcid_bool,
                                                     actlearn_sources_freqsignifs=actlearn_sources_freqsignifs,
                                                     )
        actlearn_indexes = [testdata_dict['srcid_list'].index(i) for i in class_dict['al_srcid']]
        al_arffrows = [testdata_dict['arff_rows'][i] for i in actlearn_indexes]



        out_fpath = "/home/dstarr/scratch/nomad_asas_acvs_classifier/al_iter%d_ntest%d_nal%d.dat" % (i_iter, n_test_to_sample, num_srcs_for_users)
        out_fp = open(out_fpath, 'w')
        for i, arffrow in enumerate(al_arffrows):
            out_str =  "dV: %0.3f  M: %0.3f  NOT: %0.3f  %s\n" % (class_dict['al_deltaV'][i],
                                                                class_dict['al_probis_match'][i],
                                                                class_dict['al_probis_not'][i],
                                                                arffrow)
            out_fp.write(out_str)

        out_fp.close()
        import pdb; pdb.set_trace()
        print



if __name__ == '__main__':

    pars = { \
        # From get_colors_for_tutor_sources.py:
        'fpath_train_withsrcid':"/home/dstarr/scratch/nomad_asas_acvs_classifier/train_withsrcid.arff",
        'fpath_train_no_srcid':"/home/dstarr/scratch/nomad_asas_acvs_classifier/train_no_srcid.arff",
        'fpath_test_withsrcid':"/home/dstarr/scratch/nomad_asas_acvs_classifier/test_withsrcid.arff",
        'fpath_test_no_srcid':"/home/dstarr/scratch/nomad_asas_acvs_classifier/test_no_srcid.arff",
        }


    ncaa = Nomad_Colors_Assoc_AL(pars=pars)
    ncaa.main()
